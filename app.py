import streamlit as st
import os
import cv2
import numpy as np
import google.generativeai as genai
from moviepy import VideoFileClip
import imageio_ffmpeg as im_ffmpeg
import time
import whisper
import plotly.graph_objects as go
from fpdf import FPDF
import urllib.request
import ssl

# =========================================================
# 1. Cáº¤U HÃŒNH & CSS STYLE
# =========================================================
st.set_page_config(page_title="Trá»£ lÃ½ cháº¥m Ä‘iá»ƒm", layout="wide", page_icon="ğŸ“")

st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Roboto', sans-serif; }
    .metric-card {
        background-color: #ffffff; border: 1px solid #e0e0e0; border-radius: 10px;
        padding: 20px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        transition: transform 0.2s;
    }
    .metric-card:hover { transform: translateY(-5px); box-shadow: 0 8px 15px rgba(0,0,0,0.1); }
    .metric-value { font-size: 28px; font-weight: 700; color: #1E88E5; margin-bottom: 5px; }
    .metric-label { font-size: 14px; color: #666; text-transform: uppercase; letter-spacing: 1px; }
    .main-header { text-align: center; padding-bottom: 30px; color: #0D47A1; }
    .ai-box {
        background-color: #f0f7ff; border-left: 5px solid #0D47A1; padding: 20px;
        border-radius: 5px; margin-top: 20px; font-size: 16px; line-height: 1.6;
    }
    div.stButton > button {
        background-color: #0D47A1; color: white; border-radius: 8px; padding: 0.5rem 1rem;
        font-weight: bold; width: 100%; border: none;
    }
    div.stButton > button:hover { background-color: #1565C0; color: white; }
    </style>
""", unsafe_allow_html=True)

# =========================================================
# 2. KHá»I Táº O Há»† THá»NG
# =========================================================

VISION_MODE = "None"
mp_face_mesh = None
mp_pose = None
face_cascade = None

try:
    import mediapipe as mp
    if hasattr(mp, 'solutions') and hasattr(mp.solutions, 'face_mesh') and hasattr(mp.solutions, 'pose'):
        mp_face_mesh = mp.solutions.face_mesh
        mp_pose = mp.solutions.pose
        VISION_MODE = "MediaPipe (AI Full)"
    else:
        raise ImportError("MediaPipe missing modules")
except Exception:
    try:
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        face_cascade = cv2.CascadeClassifier(cascade_path)
        VISION_MODE = "OpenCV (Motion & Face)"
    except:
        VISION_MODE = "None"

HAS_VISION = True if VISION_MODE != "None" else False

HAS_AUDIO_AI = False
try:
    import librosa
    HAS_AUDIO_AI = True
except:
    HAS_AUDIO_AI = False

current_dir = os.path.dirname(os.path.abspath(__file__))
local_ffmpeg = os.path.join(current_dir, "ffmpeg.exe")
if os.path.exists(local_ffmpeg):
    os.environ["IMAGEIO_FFMPEG_EXE"] = local_ffmpeg
    os.environ["PATH"] += os.pathsep + current_dir

# --- API KEY ---
MY_GEMINI_KEY = "AIzaSyAczYDWBottda7vVZF5gVO8kx-PRpD2-WM" 
ai_ready = False
if MY_GEMINI_KEY:
    try:
        genai.configure(api_key=MY_GEMINI_KEY)
        ai_ready = True
    except: pass

@st.cache_resource
def load_whisper_model():
    return whisper.load_model("small", device="cpu")

# =========================================================
# 3. CÃC HÃ€M PHÃ‚N TÃCH
# =========================================================

def analyze_video_comprehensive(video_path):
    if not HAS_VISION: return 0, 0
    cap = cv2.VideoCapture(video_path)
    eye_contact_frames = 0
    total_face_frames = 0
    prev_gray = None
    motion_accumulated = 0
    mesh = None
    if VISION_MODE == "MediaPipe (AI Full)":
        mesh = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)
    
    count = 0
    while cap.isOpened():
        success, image = cap.read()
        if not success: break
        if count % 10 == 0:
            total_face_frames += 1
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (21, 21), 0)
            if prev_gray is not None:
                frame_delta = cv2.absdiff(prev_gray, gray)
                thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)[1]
                motion_pixels = cv2.countNonZero(thresh)
                h, w = gray.shape
                motion_ratio = motion_pixels / (h * w)
                if motion_ratio > 0.005: motion_accumulated += 1
            prev_gray = gray

            if VISION_MODE == "MediaPipe (AI Full)" and mesh:
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image_rgb.flags.writeable = False
                results = mesh.process(image_rgb)
                if results.multi_face_landmarks:
                    for fl in results.multi_face_landmarks:
                        h, w, _ = image.shape
                        face_2d = []
                        face_3d = []
                        for idx, lm in enumerate(fl.landmark):
                            if idx in [33, 263, 1, 61, 291, 199]:
                                x, y = int(lm.x * w), int(lm.y * h)
                                face_2d.append([x, y])
                                face_3d.append([x, y, lm.z])
                        f2d = np.array(face_2d, dtype=np.float64)
                        f3d = np.array(face_3d, dtype=np.float64)
                        cam = np.array([[w, 0, w/2], [0, w, h/2], [0, 0, 1]])
                        success, rot, trans = cv2.solvePnP(f3d, f2d, cam, np.zeros((4,1)))
                        rmat, _ = cv2.Rodrigues(rot)
                        angles, _, _, _, _, _, _ = cv2.RQDecomp3x3(rmat)
                        if -15 < angles[1]*360 < 15 and -12 < angles[0]*360 < 12:
                            eye_contact_frames += 1
            elif VISION_MODE == "OpenCV (Motion & Face)" and face_cascade:
                gray_f = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray_f, 1.1, 4)
                if len(faces) > 0: eye_contact_frames += 1
        count += 1
    cap.release()
    if mesh: mesh.close()
    eye_score = (eye_contact_frames / total_face_frames * 100) if total_face_frames > 0 else 0
    gesture_ratio = (motion_accumulated / total_face_frames) if total_face_frames > 0 else 0
    gesture_score = min(gesture_ratio * 300, 100) 
    return int(eye_score), int(gesture_score)

def analyze_audio_prosody(audio_path):
    if not HAS_AUDIO_AI: return "N/A"
    try:
        y, sr = librosa.load(audio_path, duration=60) 
        rms = librosa.feature.rms(y=y)[0]
        variation = np.std(rms) / np.mean(rms)
        if variation < 0.3: return "Giá»ng Ä‘á»u"
        elif variation < 0.5: return "BÃ¬nh thÆ°á»ng"
        else: return "Truyá»n cáº£m (Tá»‘t)"
    except: return "KhÃ´ng xÃ¡c Ä‘á»‹nh"

def count_hesitations(text):
    words = text.lower().split()
    fillers = ['Ã ', 'á»', 'á»«', 'á»«m', 'lÃ ', 'mÃ ', 'kiá»ƒu', 'dáº¡']
    count = sum(1 for w in words if w in fillers)
    return count

def create_radar_chart(eye, gesture, voice_score, flow_score):
    categories = ['Giao tiáº¿p máº¯t', 'NgÃ´n ngá»¯ cÆ¡ thá»ƒ', 'Giá»ng Ä‘iá»‡u', 'Sá»± trÃ´i cháº£y']
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=[eye, gesture, voice_score, flow_score], theta=categories,
        fill='toself', name='Káº¿t quáº£', line_color='#1E88E5', fillcolor='rgba(30, 136, 229, 0.3)'
    ))
    fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 100])), showlegend=False, margin=dict(l=40, r=40, t=30, b=30), height=300)
    return fig

# --- HÃ€M Táº O PDF (Dá»° PHÃ’NG 3 Lá»šP) ---
def download_font():
    font_path = "Roboto-Regular.ttf"
    if os.path.exists(font_path): return True
    
    # 3 nguá»“n táº£i font dá»± phÃ²ng
    urls = [
        "https://fonts.gstatic.com/s/roboto/v30/KFOmCnqEu92Fr1Mu4mxK.ttf",
        "https://cdnjs.cloudflare.com/ajax/libs/roboto-fontface/0.10.0/fonts/roboto/Roboto-Regular.ttf",
        "https://raw.githubusercontent.com/google/fonts/main/apache/roboto/static/Roboto-Regular.ttf"
    ]
    
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE

    for url in urls:
        try:
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, context=ctx, timeout=5) as response, open(font_path, 'wb') as out_file:
                out_file.write(response.read())
            if os.path.exists(font_path) and os.path.getsize(font_path) > 1000:
                return True
        except: continue
    return False

def remove_accents(input_str):
    if not input_str: return ""
    s1 = u'Ã€ÃÃ‚ÃƒÃˆÃ‰ÃŠÃŒÃÃ’Ã“Ã”Ã•Ã™ÃšÃÃ Ã¡Ã¢Ã£Ã¨Ã©ÃªÃ¬Ã­Ã²Ã³Ã´ÃµÃ¹ÃºÃ½Ä‚ÄƒÄÄ‘Ä¨Ä©Å¨Å©Æ Æ¡Æ¯Æ°áº áº¡áº¢áº£áº¤áº¥áº¦áº§áº¨áº©áºªáº«áº¬áº­áº®áº¯áº°áº±áº²áº³áº´áºµáº¶áº·áº¸áº¹áººáº»áº¼áº½áº¾áº¿á»€á»á»‚á»ƒá»„á»…á»†á»‡á»ˆá»‰á»Šá»‹á»Œá»á»á»á»á»‘á»’á»“á»”á»•á»–á»—á»˜á»™á»šá»›á»œá»á»á»Ÿá» á»¡á»¢á»£á»¤á»¥á»¦á»§á»¨á»©á»ªá»«á»¬á»­á»®á»¯á»°á»±á»²á»³á»´á»µá»¶á»·á»¸á»¹'
    s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'
    s = ''
    for c in input_str:
        if c in s1: s += s0[s1.index(c)]
        else: s += c
    return s

def create_pdf_report(topic, metrics, ai_feedback, transcript):
    has_font = download_font()
    
    class PDF(FPDF):
        def header(self):
            if has_font:
                self.set_font('Roboto', 'B', 15)
                self.cell(0, 10, 'BAO CAO DANH GIA (AI PRO)', 0, 1, 'C')
            else:
                self.set_font('Arial', 'B', 15)
                self.cell(0, 10, 'BAO CAO DANH GIA (KHONG DAU)', 0, 1, 'C')
            self.ln(5)

    pdf = PDF()
    
    if has_font:
        try:
            pdf.add_font('Roboto', '', "Roboto-Regular.ttf", uni=True)
            pdf.add_font('Roboto', 'B', "Roboto-Regular.ttf", uni=True)
        except: has_font = False
            
    pdf.add_page()
    
    def safe_write(txt, style='', size=12):
        if has_font:
            pdf.set_font("Roboto", style, size)
            return txt
        else:
            pdf.set_font("Arial", style, size)
            return remove_accents(txt)

    pdf.cell(0, 10, safe_write(f"Chu de: {topic}"), 0, 1)
    pdf.ln(5)

    pdf.cell(0, 10, safe_write("1. KET QUA PHAN TICH:", 'B'), 0, 1)
    col_width = 45
    row_height = 10
    metrics_data = [
        ("Giao tiep mat", f"{metrics['eye']}%"),
        ("Ngon ngu co the", f"{metrics['gesture']}/100"),
        ("Toc do noi", f"{metrics['wpm']} wpm"),
        ("Su ngap ngung", f"{metrics['hesitation']} lan"),
        ("Giong dieu", f"{metrics['voice']}")
    ]
    for row in metrics_data:
        pdf.cell(col_width, row_height, safe_write(row[0]), border=1)
        pdf.cell(col_width, row_height, safe_write(row[1]), border=1)
        pdf.ln(row_height)
    pdf.ln(10)

    pdf.cell(0, 10, safe_write("2. DANH GIA CHI TIET TU AI:", 'B'), 0, 1)
    clean_feedback = ai_feedback.replace('*', '').replace('#', '')
    pdf.multi_cell(0, 8, safe_write(clean_feedback, size=11))
    pdf.ln(10)
    
    pdf.cell(0, 10, safe_write("3. NOI DUNG (TRANSCRIPT):", 'B'), 0, 1)
    transcript_short = transcript[:3000] + "..." if len(transcript) > 3000 else transcript
    pdf.multi_cell(0, 6, safe_write(transcript_short, size=10))

    try: return bytes(pdf.output()) 
    except: return pdf.output(dest='S').encode('latin-1', 'ignore')

# --- HÃ€M Gá»ŒI GEMINI AN TOÃ€N (FIX 404 MODEL) ---
def generate_content_safe(prompt):
    # Danh sÃ¡ch model dá»± phÃ²ng (tá»« má»›i Ä‘áº¿n cÅ©)
    models_to_try = ['gemini-2.5-flash', 'gemini-2.5-flash-latest', 'gemini-pro', 'gemini-1.5-pro']
    
    last_error = None
    for model_name in models_to_try:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            last_error = e
            continue # Thá»­ model tiáº¿p theo
            
    # Náº¿u táº¥t cáº£ Ä‘á»u lá»—i
    raise last_error

# =========================================================
# 4. GIAO DIá»†N NGÆ¯á»œI DÃ™NG 
# =========================================================

st.markdown("<h1 class='main-header'>TRá»¢ LÃ CHáº¤M ÄIá»‚M THUYáº¾T TRÃŒNH AI<br><span style='font-size: 20px; font-weight: 300; color: #555'>Trá»£ lÃ½ cháº¥m Ä‘iá»ƒm thuyáº¿t trÃ¬nh thÃ´ng minh á»©ng dá»¥ng trÃ­ tuá»‡ nhÃ¢n táº¡o</span></h1>", unsafe_allow_html=True)

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/4712/4712035.png", width=80)
    st.markdown("THIáº¾T Láº¬P THÃŠM")
    with st.expander("ğŸ“ ThÃ´ng tin ná»™i dung", expanded=True):
        target_topic = st.text_input("Chá»§ Ä‘á» chÃ­nh", "Nháº­p chá»§ Ä‘á»")
        keywords = st.text_area("Tá»« khÃ³a trá»ng tÃ¢m", "khoa há»c, mÃ´ hÃ¬nh, káº¿t quáº£, sÃ¡ng táº¡o")
    st.markdown("---")
    def status_badge(label, status):
        color = "#4CAF50" if status else "#F44336"
        icon = "âœ…" if status else "âŒ"
        st.markdown(f"<span style='color:{color}; font-weight:bold'>{icon} {label}</span>", unsafe_allow_html=True)
    status_badge(f"AI quan sÃ¡t: Sáºµn sÃ ng", HAS_VISION)
    status_badge("AI xá»­ lÃ­ Ã¢m thanh: Sáºµn sÃ ng", HAS_AUDIO_AI)
    status_badge("Káº¿t ná»‘i gemini: Sáºµn sÃ ng", ai_ready)
    st.markdown("---")
    st.caption("Developed by QuÃ ng VÄƒn Lá»£i")

with st.container():
    st.markdown("##### Táº£i lÃªn video bÃ i thuyáº¿t trÃ¬nh cá»§a báº¡n")
    uploaded_file = st.file_uploader("", type=['mp4', 'mov', 'avi'], help="Há»— trá»£ Ä‘á»‹nh dáº¡ng MP4, AVI. Tá»‘t nháº¥t dÆ°á»›i 5 phÃºt.")

if uploaded_file:
    col_vid, col_info = st.columns([1.5, 1], gap="large")
    with col_vid: st.video(uploaded_file)
    with col_info:
        st.markdown(f"**TÃªn video cháº¥m:** `{uploaded_file.name}`")
        st.info("""
        **Há»‡ thá»‘ng sáº½ phÃ¢n tÃ­ch:**
        1. ğŸ‘ï¸ **Giao tiáº¿p máº¯t:** Má»©c Ä‘á»™ tá»± tin cá»§a ngÆ°á»i trÃ¬nh bÃ y.
        2. ğŸ‘ **NgÃ´n ngá»¯ cÆ¡ thá»ƒ:** Sá»± linh hoáº¡t cá»§a tay vÃ  vai.
        3. ğŸ”‰ **Giá»ng Ä‘iá»‡u:** Cáº£m xÃºc vÃ  Ä‘iá»ƒm nháº¥n trong giá»ng nÃ³i.
        4. ğŸ“ **Ná»™i dung:** Cáº¥u trÃºc bÃ i nÃ³i theo má»¥c tiÃªu Ä‘á» ra.
        """)
        analyze_btn = st.button("Báº®T Äáº¦U CHáº¤M ÄIá»‚M NGAY", width='stretch')

    if analyze_btn:
        if not ai_ready:
            st.error("Vui lÃ²ng kiá»ƒm tra API Key Gemini!")
            st.stop()
        
        metrics = {"duration": 0, "wpm": 0, "eye": 0, "gesture": 0, "voice": "N/A", "hesitation": 0}
        temp_v, temp_a = "temp.mp4", "temp.mp3"
        
        with st.status("Äang phÃ¢n tÃ­ch video chuyÃªn sÃ¢u...", expanded=True) as status:
            st.write("Äang trÃ­ch xuáº¥t dá»¯ liá»‡u video...")
            with open(temp_v, "wb") as f: f.write(uploaded_file.getbuffer())
            
            st.write("Äang tÃ¡ch vÃ  xá»­ lÃ½ Ã¢m thanh...")
            video = VideoFileClip(temp_v)
            metrics['duration'] = video.duration
            video.audio.write_audiofile(temp_a, logger=None)
            video.close()
            
            st.write("Äang quan sÃ¡t Ã¡nh máº¯t vÃ  cá»­ chá»‰...")
            if HAS_VISION:
                metrics['eye'], metrics['gesture'] = analyze_video_comprehensive(temp_v)
            
            st.write("Äang Ä‘o lÆ°á»ng cáº£m xÃºc giá»ng nÃ³i...")
            if HAS_AUDIO_AI:
                metrics['voice'] = analyze_audio_prosody(temp_a)
                
            st.write("Äang phÃ¢n tÃ­ch ná»™i dung thuyáº¿t trÃ¬nh...")
            model = load_whisper_model()
            prompt = f"Thuyáº¿t trÃ¬nh vá» {target_topic}. Tá»« khÃ³a: {keywords}."
            result = model.transcribe(temp_a, language='vi', initial_prompt=prompt, fp16=False)
            transcript = result['text']
            
            metrics['wpm'] = int(len(transcript.split()) / (metrics['duration']/60)) if metrics['duration']>0 else 0
            metrics['hesitation'] = count_hesitations(transcript)
            
            st.write("AI Ä‘ang tá»•ng há»£p Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng...")
            gesture_text = "Ráº¥t linh hoáº¡t, sá»­ dá»¥ng tay nhiá»u" if metrics['gesture'] > 80 else ("CÃ³ cá»­ Ä‘á»™ng nhÆ°ng cÃ²n Ã­t" if metrics['gesture'] > 50 else "Äá»©ng im, thiáº¿u cá»­ chá»‰ tay")
            
            final_prompt = f"""
            Vai trÃ²: GiÃ¡m kháº£o chuyÃªn nghiá»‡p cuá»™c thi thuyáº¿t trÃ¬nh.
            Dá»¯ liá»‡u:
            - Chá»§ Ä‘á»: {target_topic}
            - VÄƒn báº£n nÃ³i: "{transcript}"
            - Tá»‘c Ä‘á»™ nÃ³i: {metrics['wpm']} tá»«/phÃºt.
            - Sá»‘ láº§n áº­m á»«: {metrics['hesitation']} láº§n.
            - TÃ´ng giá»ng: {metrics['voice']}.
            - Giao tiáº¿p máº¯t (Eye Contact): {metrics['eye']}/100.
            - NgÃ´n ngá»¯ cÆ¡ thá»ƒ (Body Language): {metrics['gesture']}/100 ({gesture_text}).
            
            YÃªu cáº§u output Ä‘á»‹nh dáº¡ng Markdown chuyÃªn nghiá»‡p:
            1. **NHáº¬N XÃ‰T CHUNG**: 2 cÃ¢u tÃ³m táº¯t tháº§n thÃ¡i, xÆ°ng tÃ´i/báº¡n.
            2. **Tá»”NG ÄIá»‚M (0-100)**: Con sá»‘ cá»¥ thá»ƒ.
            3. **ÄIá»‚M Cá»¤ THá»‚ TIÃŠU CHÃ**:
                - Ná»™i dung: /35 Ä‘iá»ƒm
                - Giá»ng Ä‘iá»‡u vÃ  ngá»¯ Ä‘iá»‡u: /15 Ä‘iá»ƒm
                - NgÃ´n ngá»¯ cÆ¡ thá»ƒ: /15 Ä‘iá»ƒm
                - Giao tiáº¿p máº¯t: /10 Ä‘iá»ƒm
                - LÆ°u loÃ¡t: /10 Ä‘iá»ƒm
                - TÃ­nh thuyáº¿t phá»¥c: /15 Ä‘iá»ƒm
            4. **ÄIá»‚M Máº NH & ÄIá»‚M Yáº¾U**: Dáº¡ng bullet point ngáº¯n gá»n.
            5. **Lá»œI KHUYÃŠN Cáº¢I THIá»†N**: 3 Ã½ chÃ­nh.
            """
            try:
                # Gá»ŒI HÃ€M AN TOÃ€N ÄÃƒ Äá»ŠNH NGHÄ¨A á» TRÃŠN
                ai_feedback = generate_content_safe(final_prompt)
                status.update(label="âœ… PhÃ¢n tÃ­ch hoÃ n táº¥t!", state="complete", expanded=False)
            except Exception as e:
                st.error(f"Lá»—i káº¿t ná»‘i AI (ÄÃ£ thá»­ má»i model): {e}")
                status.update(label="âŒ Lá»—i AI", state="error")
                st.stop()

        if os.path.exists(temp_a): os.remove(temp_a)
        if os.path.exists(temp_v): os.remove(temp_v)

        st.divider()
        st.markdown("### ğŸ“Š Káº¿t quáº£ phÃ¢n tÃ­ch")
        m1, m2, m3, m4 = st.columns(4)
        def render_card(col, label, value, unit=""):
            col.markdown(f"""
            <div class="metric-card">
                <div class="metric-value">{value}<span style="font-size:16px; color:#888">{unit}</span></div>
                <div class="metric-label">{label}</div>
            </div>
            """, unsafe_allow_html=True)
            
        with m1: render_card(st, "Giao tiáº¿p máº¯t", metrics['eye'], "%")
        with m2: render_card(st, "NgÃ´n ngá»¯ cÆ¡ thá»ƒ", metrics['gesture'], "/100")
        with m3: render_card(st, "Tá»‘c Ä‘á»™ nÃ³i", metrics['wpm'], " wpm")
        with m4: render_card(st, "Sá»± ngáº­p ngá»«ng", metrics['hesitation'], " láº§n")

        st.divider()
        c_left, c_right = st.columns([1.8, 1.2]) 
        with c_left:
            st.markdown("#### ÄÃ¡nh giÃ¡ tá»« chuyÃªn gia")
            st.markdown(f"""<div class="ai-box">{ai_feedback}</div>""", unsafe_allow_html=True)
            
        with c_right:
            st.markdown("#### Biá»ƒu Ä‘á»“ Ká»¹ nÄƒng")
            voice_map = {"Giá»ng Ä‘á»u": 50, "BÃ¬nh thÆ°á»ng": 75, "Truyá»n cáº£m (Tá»‘t)": 95, "N/A": 0, "KhÃ´ng xÃ¡c Ä‘á»‹nh": 50}
            voice_score = voice_map.get(metrics['voice'], 60)
            flow_score = max(0, 100 - (metrics['hesitation'] * 5))
            fig = create_radar_chart(metrics['eye'], metrics['gesture'], voice_score, flow_score)
            st.plotly_chart(fig, width='stretch')
            with st.expander("ğŸ“„ Xem Transcript"): st.write(transcript)
        
        st.divider()
        pdf_bytes = create_pdf_report(target_topic, metrics, ai_feedback, transcript)
        
        if download_font() == False:
            st.warning("âš ï¸ Cáº£nh bÃ¡o: Máº¡ng Ä‘ang cháº·n táº£i Font. PDF Ä‘Æ°á»£c xuáº¥t dÆ°á»›i dáº¡ng KHÃ”NG Dáº¤U Ä‘á»ƒ trÃ¡nh lá»—i.")
            
        if pdf_bytes:
            st.download_button(label="ğŸ“¥ Táº£i BÃ¡o cÃ¡o PDF", data=pdf_bytes, file_name="baocao_danhgia.pdf", mime="application/pdf")